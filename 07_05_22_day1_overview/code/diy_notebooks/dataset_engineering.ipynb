{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands On Session: Dataset Engineering\n",
    "\n",
    "### By Adi Nair, Anderson lab\n",
    "\n",
    "In this section, we'll examine some common problems in datasets that hinders classification and other tasks and think of ways to fix them. We'll use the same dataset as before from [Remedios et al., 2017](https://https://www.nature.com/articles/nature23885) \n",
    "\n",
    "### Table of contents\n",
    "\n",
    "#### 1) How well does your model perform by chance?\n",
    "Creating null distributions & chance performance for linear models\n",
    "\n",
    "#### 2) Class imbalances\n",
    "Using resampling and shuffling to overcome class imbalances\n",
    "\n",
    "#### 3) Data quality checks: Interpolation & Smoothing\n",
    "Defining metrics to evaluate the quality of recordings & handle missing entries in datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SaberaTalukder/Chen_Institute_DataSAI_for_Neuroscience/blob/main/07_05_22_day1_overview/code/diy_notebooks/dataset_engineering.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Import neccessary modules and helper functions\n",
    "# Need to delete unused packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from matplotlib import image\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import interpolate\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import imblearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Just as before let's load this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!wget https://raw.githubusercontent.com/SaberaTalukder/Chen_Institute_DataSAI_for_Neuroscience/main/07_05_22_day1_overview/data/hypothalamus_calcium_imaging_remedios_et_al.mat?raw=true\n",
    "!mv hypothalamus_calcium_imaging_remedios_et_al.mat\\?raw\\=true hypothalamus_calcium_imaging_remedios_et_al.mat\n",
    "\n",
    "remedios_data = loadmat(\"hypothalamus_calcium_imaging_remedios_et_al.mat\");\n",
    "neural_data = remedios_data['neural_data']\n",
    "attack_vector = remedios_data['attack_vector']\n",
    "sex_vector = remedios_data['sex_vector']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) How well does your model perform by chance\n",
    "\n",
    "We've previously seen that training a simple linear model to classify attack frames in this dataset fails due to the rarity of the attack label. But how bad did our models do?\n",
    "\n",
    "We need to establish a baseline performance or chance-level performance as a lower bound for what we might expect our model to achieve. \n",
    "\n",
    "A simple way to do this is by shuffling our behavior vector independently of our neural activity matrix and using that shuffled vector for classification. This needs to be performed a finite number of times to estimate true chance performance.\n",
    "\n",
    "#### Task: Shuffle attack_vector below and use that shuffled vector to perform simple linear classification below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe that even by chance, we can classify all 'not-attack' labels correctly 90% of the time! While that for attack by chance is around 0%.\n",
    "\n",
    "If you ignored class-wise performance metrics, you can see why an average performance underestimates greatly the performance for classifying attack. \n",
    "\n",
    "There are also other important considerations for making a proper chance calculation for neural data that can be taken at the level of experimental design. [Harris 2022](https://www.biorxiv.org/content/10.1101/2020.11.29.402719v3#) is a great read on this topic and you'll encounter this tomorrow with Ann Kennedy\n",
    "\n",
    "So, how can we account for the sparsity of the attack label?\n",
    "\n",
    "### 2) Class imbalance\n",
    "\n",
    "One method that allows us to fix this problem is to resample our dataset so that there are equal numbers of attack and non-attack labels. Resampling techniques are a broad class of tools that allow either random oversampling of the sparse label ('attack') or random undersampling of the majority label ('not-attack').\n",
    "\n",
    "Python once again has an easy method to implement this in the form of the [imbalanced-learn](https://imbalanced-learn.org/stable/user_guide.html) library.\n",
    "\n",
    "#### Task: Let's first perform random undersampling using [RandomUnderSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html) and then make a PCA plot using the resampled data below:\n",
    "\n",
    "#### P.S You can also implement this manually! Try that out too below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here using imbalance-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code for manual implementation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As you can see, the resampled data contains an equal amount of attack and not-attack labels. \n",
    "\n",
    "#### Task: Train a simple linear model below to seperate data in the resampled case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearly, resampling helps and our model performance has greatly improved for the attack label! \n",
    "\n",
    "But what about the baseline or chance level performance of our model after resampling? \n",
    "\n",
    "Would the chance performance of the attack label still be zero? \n",
    "\n",
    "#### Task: Calculate a shuffled performance for the resampled case above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thus, about 50% of the time, out model would accurately classify attack. Since our model performance (>90%) is much greater than the chance performance, we can be confident that the linear model has learnt to distinguish attack from not-attack labels\n",
    "\n",
    "### 2 b) Other resampling approaches\n",
    "\n",
    "Besides random-undersampling, several other undersampling approaches can be easily implemented using imblearn, find out more about that [here](https://imbalanced-learn.org/stable/references/under_sampling.html)\n",
    "\n",
    "Most deep learning packages such as PyTorch contain functions to perform resampling in their specified data structures but packages like imblearn are general purpose and are useful in many non deep learning scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Data quality checks: \n",
    "\n",
    "When dealing with neural data, it's also important to assess data quality such as noise levels in your recording. Many data pre-processing pipelines (such as kilosort for electrophysiology data and CNMFe for calcium imaging data) already perform checks on data quality but in this exercise we'll think about how to identify corrupted or noisy data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corrupted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/SaberaTalukder/Chen_Institute_DataSAI_for_Neuroscience/main/07_05_22_day1_overview/data/hypothalamus_corrupted_calcium_imaging_remedios_et_al.mat?raw=true\n",
    "!mv hypothalamus_corrupted_calcium_imaging_remedios_et_al.mat\\?raw\\=true hypothalamus_corrupted_calcium_imaging_remedios_et_al.mat\n",
    "\n",
    "remedios_data = loadmat(\"hypothalamus_corrupted_calcium_imaging_remedios_et_al.mat\");\n",
    "neural_data = remedios_data['neural_data_corrupted']\n",
    "attack_vector = remedios_data['attack_vector']\n",
    "sex_vector = remedios_data['sex_vector']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 a) Interpolation of missing data\n",
    "\n",
    "Plot the activity of neurons in neural_data below and examine it carefully. Some neurons have been corrupted to have missing entries (entries replaced by 0). Can you identify which neurons they are?\n",
    "\n",
    "#### Hint: Think of ways which can be used to identify continous 'chunks' of zeros in the activity of individual neurons, hint hint: differentiate the signal and search for periods of time where the derivative is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great! Now how do we deal with such missing values?\n",
    "\n",
    "One simple method involves [linear interpolation](https://www.cuemath.com/linear-interpolation-formula/) which can be implemented using scipy's [interp1d](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html)\n",
    "\n",
    "#### Implement interp1d to fill in missing values of neurons identified above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 b) Smoothing:\n",
    "\n",
    "In addition to missing entries, neural signals may be corrupted by noise emanating from various sources, some biological and some technical. We've intentionally corrupted some neural signals in this dataset with added gaussian noise, can you identify them?\n",
    "\n",
    "#### Plot the 4 corrupted neurons below: Hint: calculate the [signal-to-noise](https://github.com/scipy/scipy/blob/v0.16.0/scipy/stats/stats.py#L1963) ratio of each neurons and look at the top 4 neurons with highest SNR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving average filter\n",
    "\n",
    " A simple method to smooth data is to average data in time bins, this can be written as:\n",
    " $y(i)  = 1/M \\sum_{j = 0}^{M-1} x(i+j)$\n",
    "\n",
    "#### Task: Implement moving average as a function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Enter code here:\n",
    "\n",
    "def smoothMA(a,WSZ):\n",
    "    # a: NumPy 1-D or 2-D array containing the data to be smoothed (row-wise)\n",
    "    # WSZ: smoothing window size needs, which must be odd number\n",
    "\n",
    "    return aSm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does a moving average filter perform at smoothing data? \n",
    "\n",
    "#### Task: Use the moving average filter you defined to smooth neural activity for identified neurons in 3 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
