{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pArObvmICwka"
   },
   "source": [
    "# Hands On Session: Dimensionality Reduction, Principal Components Analysis (PCA), and Singular Value Decomposition (SVD)\n",
    "# By: Sabera Talukder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SaberaTalukder/Chen_Institute_DataSAI_for_Neuroscience/blob/main/07_05_22_day1_overview/code/diy_notebooks/dimensionality_reduction.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Imports - alphabetically ordered with shortcuts\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from numpy.linalg import svd\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hint: do not reinvent the wheel! If you want to do something, a preexisiting package, library, function, etc. exists to do what you want. Google & Stack Overflow are your friends üòÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "### Dataset background: today we'll be working with calcium imaging data from one male mouse. We have already converted the calcium imaging videos into continuous neural signals, so you don't have to worry about it (you're welcome üòò). The male mouse has different visitors in his cage throughout the recording, and we'll explore dimensionality reduction by determining if it's üê≠‚ù§Ô∏è or üê≠ üò°!\n",
    "\n",
    "#### Let's start by loading in our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/SaberaTalukder/Chen_Institute_DataSAI_for_Neuroscience/main/07_05_22_day1_overview/data/hypothalamus_calcium_imaging_remedios_et_al.mat?raw=true\n",
    "!mv hypothalamus_calcium_imaging_remedios_et_al.mat\\?raw\\=true hypothalamus_calcium_imaging_remedios_et_al.mat\n",
    "\n",
    "hypothalamus_data = loadmat('hypothalamus_calcium_imaging_remedios_et_al.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many data arrays are contained in hypothalamus_data?\n",
    "#### Hint: what happens if you type the variable name in a cell and run the cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the N data arrays into N separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the dimensionality of each of the N data arrays?\n",
    "## What do you think the dimensions represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the distributions of each of the N data arrays as a histogram!\n",
    "#### Hint: the answer to this question can be a picture!\n",
    "#### Hint Hint: sometimes functions run faster if you transform a matrix a vector first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the N data arrays.\n",
    "#### Hint: sometimes the most expeditious way to visualize data is to treat it as an image!\n",
    "#### Hint Hint: one visualization might give you something you dont expect, but is the problem the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do the values inside the arrays represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter answer here (code can be used, but not required):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great job exploring the data! Now let's dive into what we can do with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prinicpal Components Analysis (PCA)\n",
    "#### We're going to dive into how PCA works, but first we're going to see what can be done with it! All you need to know for now is that PCA creates a lower dimensional representation of your data to preserve the data's variance.\n",
    "\n",
    "#### By now you know you have a neural data array that is number_of_neurons by time, let's say the dimensionality is NxT. What we are going to do with PCA is take all of our time steps and compress them; this will output an array that is SxT where S < N. In other words each time step is initially an N dimensional vector, that gets compressed into an S dimensional vector where S < N. Let's explore this with S = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a PCA model with with S = 3\n",
    "pca_model_s_3 = PCA(n_components=3)\n",
    "\n",
    "# STOP & Check Yourself: Do you know why we can just call \"PCA\"?\n",
    "\n",
    "# with the PCA model instance we created to our neural data\n",
    "neural_pca_s_3 = pca_model_s_3.fit_transform(neural_data.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the dimensionality of the PCAed neural data? What do these dimensions mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Principal Components (aka PCs) in 3D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Enter code here:\n",
    "pc1 = pass\n",
    "pc2 = pass\n",
    "pc3 = pass\n",
    "\n",
    "ax.scatter3D(pc1, pc2, pc3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nice job! Now rotate your representation! What interesting things do you notice about your dimensionality reduced data?\n",
    "#### Hint: why does this data look connected?\n",
    "#### Hint Hint: Why are the axes so different from each other? What do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this to switch out of movable 3d plotting (i.e. when you have 2d plots next)\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're going to return to this visualization, but first you have to be thinking to yourself, we got rid of A LOT of dimensions how do we know this representation is still good? Great question! You tell me üëáüèªüëáüèºüëáüèΩüëáüèæüëáüèø"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much variance is explained by each of these top 3 principal components? What does this tell you about the data?\n",
    "#### Hint: what is the first hint I gave you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that you know how much variance is explained by each of the top 3 PCs, let's explore the representation we built further!\n",
    "\n",
    "#### Let's start by coloring each time point as a function of when it appears in the time series.\n",
    "#### Hint: you're not changing the plot you're changing the color!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this tell you about the representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now make three separate plots colored by the attack variable! üò°üê≠‚ùì\n",
    "\n",
    "### For plot 1: Plot only the attack data points\n",
    "### For plot 2: Plot only the other data points\n",
    "### For plot 3: Plot the the attack data points on top of the other data points\n",
    "\n",
    "#### Hint: it may be easier to separate your data by labels first!\n",
    "#### Hint Hint: for plot 3 play with opacity (goes by a different name though!), and zorder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now build the same plot but color based on the mouse sex variable! ‚ù§Ô∏èüê≠‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great! Now that you know more about the data and PCA, I want you to repeat everything you just did if you reduce the data to 2 PCs! More explicitly:\n",
    "\n",
    "#### ‚ú¶ Train a model on the neural data with 2 PCs.\n",
    "#### ‚ú¶ How much explained variance do these 2 PCs capture? Do you notice anything interesting about these 2 PCs? üòâ\n",
    "#### ‚ú¶ How is time visualized in these 2 PCs?\n",
    "#### ‚ú¶ How is üê≠ üò° visualized in these 2 PCs?\n",
    "#### ‚ú¶ How is üê≠ ‚ù§Ô∏è visualized in these 2 PCs?\n",
    "\n",
    "### Finally, if you needed to build a model to classify time, attack, or the visitor's sex how many PCs would you use? Do you lose anything between 3 PCs and 2PCs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement PCA yourself!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, mean center your data!\n",
    "##### The reason we didn't have to do this before is because the PCA function we called automatically did this for us üò±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now verify that the pca function returns the same thing in 2D for the not mean centered data and the mean centered data to prove that function we call automatically does this for us. Color using the time steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have mean centered data we can transform our data via two paths:\n",
    "### (1) By stepping through linear decomposition ourselves.\n",
    "### (2) By using singular value decomposition (a.k.a SVD).\n",
    "\n",
    "#### Let's start with path (1):\n",
    "#### First calculate the covariance matrix of your mean centered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "##### Hint: Make sure you've sorted the eigenvalues and eigenvectors to be in either ascending or descending order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now project your mean centered data into a reduced space using the 2 largest eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot your transformed data using time as your color!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok but why is the representation flipped?!\n",
    "#### PCA is sign invariant, meaning that we can multiply the axes by -1 and the interpretation of the dimensionality reduced space stays the same. \n",
    "#### Now that we know this is true, change your plot to look like the plot when we use the PCA library directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Job!! Now you've calculated PCA all by yourself using matrix operations ü§©ü§©ü§© Let's move on to implementing PCA using SVD (singular vector decomposition).\n",
    "\n",
    "#### Singular vector decomposition is a method that decomposes a matrix into three matricies. U, S, and Vt. The left singular vectors are the columns of U. S are the singular values. V is a matrix whose columns are the right singular vectors. Vt is the transpose of V. Our input data (call it X) equals U\\*S\\*Vt ‚û°Ô∏è X = U\\*S\\*Vt.\n",
    "\n",
    "#### We're not going to implement svd ourselves. Please run np.linalg's svd on our mean centered data.\n",
    "##### Hint: have we already loaded svd?\n",
    "##### Hint Hint: run with full_matrices = False otherwise it might take you a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the dimensions of U, S, and Vt? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because we used the data we want transformed to calculate U, S, and Vt, we can directly multiply our left singular vectors and singular values together to get our transformed data.\n",
    "\n",
    "#### Hint: the singular values need to be converted into a diagonal matrix to make the matrix multiplication easier.\n",
    "#### Hint Hint: We only want to transform our data to a reduced dimension of 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot your transformed data to match our PCA library plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To dive deeper into the math behind PCA & SVD stay tuned for day 3!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Notebook2_Overfitting_Regulaization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
