{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pArObvmICwka"
   },
   "source": [
    "# Hands On Session: Uncovering brain-behavior relationships and overcoming overfitting with regularization\n",
    "\n",
    "### by Adi Nair, Anderson Lab\n",
    "\n",
    "In this section, we'll use tools learnt in Section 1 to perform dimensionality reduction such as PCA also perform decoding of behavior from neural activity using linear and logistic regression, going over important concepts such as regularization and overfitting. We'll also implement linear regression from scratch and learn about closed-form solutions. We'll be using data from the ventromedial hypothalamus (abbreviated VMHvl) from a male mouse interacting with females and males from [Remedios et ., 2017, Nature](https://https://www.nature.com/articles/nature23885)\n",
    "\n",
    "üê≠ üê≠ üê≠ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SaberaTalukder/Chen_Institute_DataSAI_for_Neuroscience/blob/main/07_05_22_day1_overview/code/diy_notebooks/overfitting_regularization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "#### 0) How does your data look like?\n",
    "Simple exercises to plot neural and behavior data \n",
    "\n",
    "#### 1) Dimensionality reduction\n",
    "Using sklearns' PCA to observe seperation of behavior in activity space\n",
    "\n",
    "#### 2) Can you predict behavior from neural data?\n",
    "#### 2 a) Train test splits \n",
    "A crucial process for any analysis\n",
    "\n",
    "#### 2 b) Performing linear regression\n",
    "Using sklearn's LinearRegression without any penalties\n",
    "\n",
    "#### 2 c) Prediction and quatifying performance\n",
    "Using confusion matrices to observe precision, recall & F1 scores\n",
    "\n",
    "#### 3 a) Overcoming overfitting: LASSO or L2 Norm\n",
    "Using LassoCV to perform L2 reg.\n",
    "\n",
    "#### 3 b) Lasso model performance\n",
    "\n",
    "#### 3 c) Overcoming overfitting: Ridge or L1 Norm\n",
    "Using RidgeCV to perform L1 reg.\n",
    "\n",
    "#### 3 d) Ridge model performance\n",
    "\n",
    "#### 4) Class imbalance\n",
    "A crucial step in classification and decoding problems\n",
    "\n",
    "#### 5) Logistic regression\n",
    "A better classification tool using sklearn's LogisticRegression\n",
    "\n",
    "#### 6 a) Digging deeper and implementing regression from scratch\n",
    "Linear regression has a closed form solution, meaning that it can be solved mathematically without optimization. Write out the math here!\n",
    "\n",
    "#### 6 b) Testing our regression function\n",
    "\n",
    "#### 7) Implementing regularization from scratch\n",
    "Add regularization to the closed form solution\n",
    "\n",
    "#### 8) Partial least squares regression\n",
    "Find a latent dimension correlated with behavior here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "sbfQ8k1hHZsS"
   },
   "outputs": [],
   "source": [
    "# Import neccessary modules and helper functions\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from matplotlib import image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import scale \n",
    "\n",
    "def forceAspect(ax,aspect):\n",
    "    im = ax.get_images()\n",
    "    extent =  im[0].get_extent()\n",
    "    ax.set_aspect(abs((extent[1]-extent[0])/(extent[3]-extent[2]))/aspect)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt_7D6PyCvVz"
   },
   "source": [
    "### But first!  üê≠ \n",
    "\n",
    "A bit more about our dataset: the data you examined in part 1 comes from a head-mounting microendoscope which collected calcium imaging (GCaMP6s) data from a male mouse enaging in either mating behavior towards a female or aggressive behavior towards a male. Here we are specifically recording from estrogen 1 receptor expressing neurons in the ventromedial hypothalamus (VMHvl).\n",
    "\n",
    "After collection of the imaging data, a data extraction pipeline is used to identify cells and extract time evolving traces from the imaging video. Finally, experimenters perform computer-vision assisted annotation of behaviors to give you the behavior label binary vectors you are using today\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1ts_czT8HluZHqnr1cIUj1BNkwRfjWZoj\n",
    "\" alt=\"EMFigure\" width=\"500\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Just as before let's load this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Rbi1KUepqn7-",
    "outputId": "ee3ed549-748f-4b06-e222-34c976199c0e"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!wget https://raw.githubusercontent.com/SaberaTalukder/Chen_Institute_DataSAI_for_Neuroscience/main/07_05_22_day1_overview/data/hypothalamus_calcium_imaging_remedios_et_al.mat?raw=true\n",
    "!mv hypothalamus_calcium_imaging_remedios_et_al.mat\\?raw\\=true hypothalamus_calcium_imaging_remedios_et_al.mat\n",
    "\n",
    "remedios_data = loadmat(\"hypothalamus_calcium_imaging_remedios_et_al.mat\");\n",
    "neural_data = remedios_data['neural_data']\n",
    "attack_vector = remedios_data['attack_vector']\n",
    "sex_vector = remedios_data['sex_vector']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) How does your data look like? \n",
    "\n",
    "Now that we've examined neural activity of VMHvl in session 1, let's plot both a behavior raster showing attack behavior (present in attack_vector), intruder sex (present in sex_vector) and neural activity together. \n",
    "\n",
    "Attack is a behavior causally linked to VMHvl activity: activation this hypothalamus brain region using optogenetic can produce aggression (see: [Lin et ., 2011, Nature](https://www.nature.com/articles/nature09736) for some cool videos)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Task: Generate a plot showing the behavior annotation array attack_vector, sex_vector along with neural activity below:\n",
    "\n",
    "##### hint: use [imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) as in section 1! Also try playing with the 'aspect' parameter to make the plots readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7279e9qsD94s"
   },
   "source": [
    "### 1) Dimensionality reduction!\n",
    "### 1 a) How seperable is behavior in a pca space?\n",
    "\n",
    "Great! Let's now perform PCA as in section 1 and examine how sex is represented in two PCs. PCA can be easily performed in a few lines of code using sklearn's [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) function. This function has already be imported for you! Click the link to examine some example calls of the function and then perform PCA below:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Task: Generate a PCA plot colored by the behavior annotation array sex_vector using [scatter](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html) below! \n",
    "Hint: Scatter also accepts a parameter to color your plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Success! You've recreated a key observation in [Remedios et al, 2017; see Fig 1k](https://www.nature.com/articles/nature23885/figures/1) \n",
    "Intruder sex is easily seperable within a 2D PCA space for VMHvl! \n",
    "What about other behavior? \n",
    "\n",
    "#### Task: Let's plot the same PCA plot but now color by the attack_vector instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Can you predict behavior from neural data?\n",
    "\n",
    "Attack behavior is fairly rare in our dataset, let's train a simple linear regression model to estimate if attack can be decoded from this population before we move to methods like partial least squares.\n",
    "\n",
    "In regression, we predict the behavior as a linear combination of the activity of neurons, as follows: $y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 ... + \\beta_n x_n$.\n",
    "\n",
    "where $y$ is the behavior we predict and $x_1, x_2, ... , x_n$ are the activities of individual neurons at a given time point.\n",
    "\n",
    "### 2 a) Make a Train Test Split\n",
    "\n",
    "The first step involves splitting data into train & test splits. As a first pass, let's split the last third of the data into the test set.\n",
    "\n",
    "#### Task: Generate a train-test split in your dataset below (hint: use the function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from sklearn)\n",
    "\n",
    "#### For this exercise, set the shuffle parameter to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 b) Performing regression\n",
    "\n",
    "We will use sklearn's [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) module to perform the actual regression. \n",
    "\n",
    "This can be implemented in just a single line of code thanks to sklearn! \n",
    "\n",
    "#### Task: Generate a regression model using sklearn's [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 c) How well is your model performing?\n",
    "\n",
    "To quantify performance, we can use a confusion matrix with quantifies the degree of false positives and false negative for each class (attack and not-attack). These metrics can then be used to calculate an accuracy and precision value for each class. We use sklearn's [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) module to perform this. \n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1MUZDHf9xFvyBNrGGAvlo1scNQwzIu0-1\n",
    "\" alt=\"EMFigure\" width=\"500\"><center>\n",
    "\n",
    "Keep in mind, the output of regression will be a number, not a class label. This output represents a probability (but can be negative since linear regression does not impose non-negativity). We need to threshold the output by a value (say 0.4 as an example) to generate a binary signal that be compared with attack_vector.\n",
    "<br>\n",
    "    \n",
    "#### Use sklearn's [predict](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) and [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to quantify performance of your linear model\n",
    "\n",
    "Note: confusion matrix by default provides counts, this needs to be normalized either row-wise or column-wise. Try both below. Which is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach reveals perhaps the most common problem faced in ML, namely overfitting. Here, the train performance of the linear model suggests that model has learnt to distinguish attack frames, however when applied on a held out test dataset, the model performance collapses.\n",
    "\n",
    "Let's also observe this as a behavior raster:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Task: Create a plot showing both the predicted behavior raster (the binary vector obtained after thresholding regression output) and the actual behavior raster for attack (attack_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 ) How can we overcome overfitting?\n",
    "\n",
    "### 3 a) LASSO or L2 Regularization\n",
    "\n",
    "To overcome overfitting, we apply regularization, a method where a penalty is introduced to curb overfitting. Different types of regularization exist, here we will apply LASSO (least absolute shrinkage and selection operator), a method that allows us to enforce sparsity in predicted weighted from linear regression. LASSO is also known as L2 norm regularization.\n",
    "\n",
    "Most methods in machine learning always involving comparing a data point with it's predicted value. This can be written as: $y - \\beta x $ in the case of linear regression. The goal of machine learning is to estimate a value of $\\beta$ that minimizes this error that we also call a loss function. In regularization, we apply a penalty to this error, which for LASSO is as follows:\n",
    "\n",
    "$(y - \\beta x) + \\alpha \\sum_{i = 1}^{p} |\\beta_i|$\n",
    "\n",
    "The value of lamba specifies the amount of regularization. Lasso created sparse weights, if a variable has no predictive power in the regression problem, Lasso applies a zero weight to such variables. How lasso achieves this can be understood from a geometric interpretation of weights, read more about it [here](). The strength of sparisity depends on the value of $\\alpha$\n",
    "\n",
    "### How do we select the right model?\n",
    "\n",
    "The value of lamba is selected by cross-validation, a process where the training data is split into several 'folds' or portions and the model is trained on some folds and tested on the held-out fold. This is a process called model selection and is vital to most ML methods.\n",
    "\n",
    "Lasso can be implemented using sklearn's [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). \n",
    "\n",
    "#### Task: Read the help sklearn [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) help page and generate a lasso model with cross validation below. Make a plot showing model performance against $\\alpha$ value using cross validation (called model selection plot) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 b) Lasso model performance\n",
    "\n",
    "#### Task: Use the code you wrote earlier to create predict data on the held out test set and create confusion matrices below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's also visualize the predicted and actual ocurrences of attack behavior in the test set below\n",
    "\n",
    "#### Task: Generate a behavior plot for both predicted and actual occurence of attack below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 c) Overcoming overfitting: Ridge or L1 Norm\n",
    "\n",
    "#### Oh no! LASSO isn't sufficient üòû \n",
    "\n",
    "No worries, let's try another type of reglurization! Like LASSO (or L2 norm), a second type of regularization called Ridge (or L1 norm) might be useful here!:\n",
    "\n",
    "In LASSO, we added a penalty to our regression problem as follows:\n",
    "\n",
    "LASSO or L2 norm: $(y - \\beta x) + \\alpha \\sum_{i = 1}^{p} |\\beta_i|$\n",
    "\n",
    "In Ridge, we add a slightly different penalty: \n",
    "\n",
    "Ridge or L1 norm: $(y - \\beta x) + \\alpha \\sum_{i = 1}^{p} \\beta_i^2 $\n",
    "\n",
    "Unlike Lasso which creates sparsity, ridge scales weights and reduces their overall value.\n",
    "\n",
    "Explain more about Ridge regression [here]()\n",
    "\n",
    "#### Let's implement Ridge Regression using sklearn's RidgeCV method, once again the $\\alpha$ parameter is determined by cross validation\n",
    "\n",
    "\n",
    "#### Note: unlike LassoCV, we need to provide a list of $\\alpha$ values for Ridge CV. As a first example, try values from 0-200 in intervals of 1.\n",
    "\n",
    "#### Task: Perform Ridge regression and create a model selection plot showing model performance against values for  $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 d) Ridge model performance\n",
    "\n",
    "#### Use the code you wrote earlier to create predict data on the held out test set and create confusion matrices below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Class Imbalance\n",
    "\n",
    "#### okay... so we're doing a little better than before, but still not perfect!\n",
    "\n",
    "### Notice something about our data, our train & test splits contain very few examples of the label 'attack'! This problem called [class imbalance]() is very detrimental to our problem! \n",
    "\n",
    "There are two ways to fix this: We can either shuffle our data to ensure that the 'attack' label is uniformly sprinkled throughout the data or we can sub-sample our data to include equal numbers of attack and not-attack labels. We'll try the latter approach in session 3 today, for now let's try the former\n",
    "!\n",
    "Hint: It's as simple as including an argument in the function train_test_split\n",
    "\n",
    "#### Task: Redo regression (either Ridge or LASSO), but now including shuffled or sub-sampled classes below\n",
    "#### For shuffling data to uniformly distribute attack labels, add shuffle=True when calling train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAY!! Our decoder works!!  üéâüéâüéâ\n",
    "\n",
    "Let's finally make a plot showing our entire dataset: the attack annotation, neural data and predicted attack labels on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Can we get better at predicting behavior?\n",
    "\n",
    "### 5 a) Logistic Regression\n",
    "\n",
    "Linear regression is a great simple tool to quantifying correlations between neural activity and behavior. However, the output of regression wasn't a class label. Other methods such as logistic regression (and many others such as SVMs & Random Forests that we will cover in later tutorials) are better suited to classification problems.\n",
    "\n",
    "Logistic regression adds a non-linear step between your prediction and coefficients. We start with a linear function: $f(x) = \\beta_1 x_1 + \\beta_2 x_2 ... + \\beta_n x_n $ which is then passed into a sigmoid function $p(x) = 1/(1+exp(-f(x))$. Thus the output of logistic regression is a probability between 0-1 of the prediction. This can be thresholded to produce a class label.\n",
    "\n",
    "sklearn makes it very easy to perform Logistic regression using [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) that performs a built-in regularization step. Methods like logistic regression are more powerful than Linear regressison and can find an accurate solution sometimes even in the presence of a class imbalance\n",
    "\n",
    "#### Note: Unlike linear regression, logistic regression directly gives us a label as output. Thresholding isn't required by default\n",
    "\n",
    "#### Task: Implement Logistic Regression using sklearn below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does linear regression work?\n",
    "\n",
    "### 6 a) Digging deeper and implementing regression from scratch:\n",
    "\n",
    "We've implemented linear regression with sklearn's helpful functions, but how does regression actually work?\n",
    "\n",
    "Let's gain an understanding by coding out the steps behind regression.\n",
    "As we saw earlier, the goal of regression is to predict a variable y (behavior) as a linear combination of x (neural activity). This can be written as:\n",
    "\n",
    "$y = \\beta X$\n",
    "\n",
    "Notice here that we switch to matrix notation, here $\\beta$ is no longer a single number but a vector as long as the number of neurons (n). Hence it's dimensionas as $(1 x n)$. X here is also a matrix with dimensions $(n x t)$, where t is the number of time points.\n",
    "\n",
    "There are many algorithms that allow us to find the $\\beta's$ that make our weights. Some methods are iterative, using a loss function as defined earlier and reducing error over time. Luckily, linear regression also has an analytical solution which can be implemented in a single step!\n",
    "\n",
    "The solution here is simply: \n",
    "\n",
    "$\\beta = (X^{T} X)^{-1} X^{T} y$\n",
    "\n",
    "Why is that?? We'll explain later but if you're curious you can read about the mathematical steps [here](https://dustinstansbury.github.io/theclevermachine/derivation-normal-equations):\n",
    "\n",
    "Let's try this out!\n",
    "\n",
    "We're going to use [Object Oriented Programming](https://en.wikipedia.org/wiki/Object-oriented_programming) here and create a class which can implement this solution for us \n",
    "\n",
    "#### Fill in the function below to implement linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here\n",
    "\n",
    "class LinearRegClosedSolution(object):\n",
    "    \n",
    "    def __int__(self):\n",
    "        self.betas = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X = self._concatenate_ones(X)\n",
    "        self.betas = # Enter equation for beta here\n",
    "        \n",
    "    def predict(self, X):\n",
    "        b = self.betas\n",
    "        prediction = # Enter equation for finding solution here\n",
    "        return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 b) Testing our regression function!\n",
    "\n",
    "Now that we've implemented linear regression, let's test it out! \n",
    "\n",
    "Split the data into train and test folds and use the class created above to implement linear regression\n",
    "\n",
    "#### Try that below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code to perform regression here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now predict the test data using our closed form function and generate a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Yay! Our closed form regression works üéâüéâüéâ\n",
    "\n",
    "Let's also try fitting a regularized version of the closed form solution.\n",
    "\n",
    "Ridge regression is a form of regularization that possess an analytical solution which is as follows:\n",
    "\n",
    "$\\beta = (X^{T} X + \\lambda I)^{-1} X^{T} y$\n",
    "\n",
    "where $\\lambda$ is a parameter we need to determine using cross validation once more!\n",
    "\n",
    "Let's implement a function to perform ridge regression below:\n",
    "\n",
    "#### Implement a ridge regression class below where you can feed in a single regularization parameter $\\lambda$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here\n",
    "\n",
    "\n",
    "class LinearRegRidgeClosedSolution(object):\n",
    "    def __int__(self):\n",
    "        self.betas = []\n",
    "          \n",
    "    def fit(self, X, y, regLamba):\n",
    "        X = self._concatenate_ones(X)\n",
    "        self.betas = # Enter equation for beta here\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Enter equation for finding solution here\n",
    "        return prediction\n",
    "    \n",
    "    def _concatenate_ones(self, X):\n",
    "        ones = np.ones(shape = X.shape[0]).reshape(-1,1)\n",
    "        return np.concatenate((ones,X),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 b) Testing our regression function!\n",
    "\n",
    "Now that we've implemented linear regression with regularization, let's test it out! \n",
    "\n",
    "#### Split the data into train and test folds and use the class created above to implement linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now predict the test data using our closed form function and generate a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) What if you want to perform dimensionality reduction and regression jointly?\n",
    "\n",
    "### 8 a) cue Partial least squares regression (PLS)!\n",
    "\n",
    "PLS acts as both a dimensionality reduction method and a classification tool. It decomposes neural data into a given number of factors and then uses those factors for classifying a particular variable such as a behavior. \n",
    "\n",
    "sklearn once comes to the rescue with a built in implementation in the form of [PLSRegression](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html).\n",
    "\n",
    "PLSRegression is applied as any other regression technique. You can reuse your code above to generate train, test splits and perform cross validation using [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) to CV your model. After classification, use the model.x_scores_ to access the latent representations that are most prediction of the behavior here!\n",
    "\n",
    "#### Task: Perform PLS and plot classification performance below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike linear regression, PLS attempts to find dimensions that maximize variance as well as their correlation to an external variable such as behavior. Thus, the PLS model performance comes close to regression but is not as high as it. \n",
    "\n",
    "What do the PLS dimensions look like? This can be found in parameter - pls.x_scores_\n",
    "\n",
    "### As the final part of this notebook, run PLS on the entire dataset and plot the 1st PLS component along with a behavior raster and neural data and also plot the output of a regression method (such as ridge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Notebook2_Overfitting_Regulaization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
