{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f1dd5d",
   "metadata": {},
   "source": [
    "# Hands On Session: Autoencoders and Machine Learning\n",
    "## By: Sabera Talukder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa7222",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SaberaTalukder/Chen_Institute_DataSAI_for_Neuroscience/blob/main/07_08_22_day4_autoencoders_and_ML_introduction/code/diy_notebooks/neural_autoencoders.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c3ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Imports - alphabetically ordered with shortcuts\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEED = 38\n",
    "np.random.seed(SEED)\n",
    "_ = torch.random.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a923da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment code below to pull directly from github\n",
    "# !wget https://github.com/SaberaTalukder/Chen_Institute_DataSAI_for_Neuroscience/blob/main/07_08_22_day4_autoencoders_and_ML_introduction/data/hypothalamus_calcium_imaging_remedios_et_al.mat\n",
    "# !mv hypothalamus_calcium_imaging_remedios_et_al.mat\\?raw\\=true hypothalamus_calcium_imaging_remedios_et_al.mat\n",
    "# hypothalamus_data = loadmat('hypothalamus_calcium_imaging_remedios_et_al.mat')\n",
    "\n",
    "path = '/Users/A*****/Desktop/sabera_chen/Chen_Institute_DataSAI_for_Neuroscience/07_08_22_day4_autoencoders_and_ML_introduction/data/'\n",
    "hypothalamus_data = loadmat(path + 'hypothalamus_calcium_imaging_remedios_et_al.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d1c65",
   "metadata": {},
   "source": [
    "## Throwback!! ðŸ˜ŽðŸ˜ŽðŸ˜Ž This is the same neural data that we worked with on our first day!\n",
    "#### Since you know so much about it please pull out the neural data array from they hypothalamus data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709cf473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d1ba0",
   "metadata": {},
   "source": [
    "## Now shorten this array, and for every neuron take only the first 1000 time steps\n",
    "#### Hint: You should be able to do this in one line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cc7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e8149",
   "metadata": {},
   "source": [
    "## Now run PCA on the shortened neural data, color according to time!\n",
    "#### Hint: It's very similar to the dimensionality reduction code from the first day, but you're only doing it on 1000 time step data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f202dcaf",
   "metadata": {},
   "source": [
    "# Now, I'm going to give you the code for the custom dataset class, but I'm going to ask you questions so read each line in depth!\n",
    "\n",
    "### A custom dataset class allows us to load our data into a pytorch data loader. This is important when building pytorch models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.data[idx, :]\n",
    "        sample = {\"data\": instance}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c0327",
   "metadata": {},
   "source": [
    "### Because we are dealing with pytorch, it's best to pass in our data as a tensor of floats.\n",
    "#### A float is a 'floating-point number', it's just a number with decimals out to some level of precision. \n",
    "#### A tensor is a container that stores data in N dimensions. A matrix is a special case of a tensor that is 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74da93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_data_float_tensor = torch.tensor(neural_data_short).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df8010",
   "metadata": {},
   "source": [
    "#### What is the shape of our tensor of floats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47e3ef",
   "metadata": {},
   "source": [
    "## Now apply the CustomDataset class to our neural data tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a015de7",
   "metadata": {},
   "source": [
    "## What happens if you call __len__() on the data? What does this length represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe643869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ebd237",
   "metadata": {},
   "source": [
    "## How about __getitem__(idx)? What is the idx variable that __getitem__() is indexing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869047e6",
   "metadata": {},
   "source": [
    "## Now we are going to break our data into a train (90% of the data) and test (10% of the data) split\n",
    "\n",
    "#### How many neurons should be in the train set? How about the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c172d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129443e",
   "metadata": {},
   "source": [
    "## Now using the torch.utils.data.random_split() function split the data into a train set and a test set\n",
    "#### Hint: Google is your friend ðŸ¤—\n",
    "#### Hint Hint: You should be able to do this in one line of code ðŸ˜±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73d212",
   "metadata": {},
   "source": [
    "## Now using the torch.utils.data.DataLoader() make a train data loader and a test data loader.\n",
    "#### Make sure to use the dataset, batch_size, and shuffle parameters when you call the function.\n",
    "#### For simplicity, set the batch size to be larger than all the data you have. This isn't practical for large datasets, but with our small data it will work great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767027a",
   "metadata": {},
   "source": [
    "## Enumerate through your train_loader and test loader is the index, and data what you expect to see?\n",
    "#### Hint: Enumerate is an important word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72cca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd18f1",
   "metadata": {},
   "source": [
    "# Woot Woo!! Nicely Done ðŸ˜Ž Now we're going to build our very first neural network!\n",
    "\n",
    "### There are 3 steps! Make sure to do them all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa62346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "          \n",
    "        # Implementing a linear encoder.\n",
    "        # Each layer is composed of a linear layer followed by a Relu activation function.\n",
    "        # The last layer is just a linear layer!\n",
    "        # We take a data point from a dimension of 1000, to 100, to 4.\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            # STEP 1: Enter code here\n",
    "        )\n",
    "          \n",
    "        # Implementing a linear decoder.\n",
    "        # Each layer is composed of a linear layer followed by a Relu activation function.\n",
    "        # The last layer is just a linear layer!\n",
    "        # We take a data point from a dimension of 4, to 100, to 1000.\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            # STEP 2: Enter code here\n",
    "        )\n",
    "        \n",
    "    # You first want to pass your input data through the encoder. This creates an embedding.\n",
    "    # You then want to pass this embedding through your decoder. This creates your reconstruction.\n",
    "    # You then wan your forward function to return your reconstruction.\n",
    "    def forward(self, x):\n",
    "        # STEP 3: Enter code here\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf10f87",
   "metadata": {},
   "source": [
    "# Amazing work! Now that you've built an autoencoder, let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 5\n",
    "outputs = []\n",
    "losses = []\n",
    "validation_outputs = []\n",
    "validation_losses = []\n",
    "\n",
    "# Model Initialization\n",
    "model = Autoencoder()\n",
    "# Using Mean-Squared-Error MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2, weight_decay = 1e-8)\n",
    "# learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=0.98)\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # training on train set\n",
    "    model.train()\n",
    "    \n",
    "    # Loop through your training data\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # STEP 1: pull out the data from your batch        \n",
    "        # STEP 2: get the reconstructed data from the Autoencoder Output\n",
    "        # STEP 3: calculate the loss function between the reconstrucion and original data\n",
    "        \n",
    "        # set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # the gradient is computed and stored\n",
    "        loss.backward()\n",
    "        # perform the parameter update\n",
    "        optimizer.step()\n",
    "        # Storing the losses in a list for plotting\n",
    "        losses.append(float(loss.detach()))\n",
    "       \n",
    "    # put model into evaluation mode\n",
    "    model.eval()\n",
    "    # loop through your testing/validation data\n",
    "    for validation_batch_idx, validation_batch in enumerate(test_loader):\n",
    "        pass\n",
    "        # STEP_4: pull out the data from your validation batch\n",
    "        # STEP 5: get the reconstructed data from the Autoencoder Output\n",
    "        # STEP 6: calculate the loss function between the reconstrucion and original data        \n",
    "        # STEP 7: append the validation losses to the validation loss list\n",
    "        \n",
    "    # STEP 8: append the outputs to the train outputs lists in the form of:\n",
    "    # (epochs, original data, reconstruction). Don't forget to transform your tensors into numpy arrays!!!\n",
    "    \n",
    "    # STEP 9: append the outputs to the validation outputs lists in the form of:\n",
    "    # (epochs, original data, reconstruction). Don't forget to transform your tensors into numpy arrays!!!    \n",
    "\n",
    "    print('Finished Epoch: ', epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d96f1",
   "metadata": {},
   "source": [
    "## What is the difference between the training loop and the validation loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0644e",
   "metadata": {},
   "source": [
    "## What is the length of your ouputs? What does this correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c709888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee1cf5",
   "metadata": {},
   "source": [
    "## For the last epoch, what are each of items in that tuple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda25e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb55254",
   "metadata": {},
   "source": [
    "## For the last epoch, and a neuron (you pick the number) plot the original data time series and the reconstruction time series for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4940a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b211bb0",
   "metadata": {},
   "source": [
    "## For the last epoch and all the neurons at once, plot the original training data and the reconstructed training data. How do you think it looks?\n",
    "#### Hint: This of this represenation as an image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fa24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ed191",
   "metadata": {},
   "source": [
    "## For the last epoch and all the neurons at once, plot the original testing data and the reconstructed testing data. How do you think it looks?\n",
    "#### Hint: This of this represenation as an image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1667939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f9ed22",
   "metadata": {},
   "source": [
    "## Plot the loss curves for the training and validation on top of one another in two different colors! Given what we've learned about loss curves, is the model done training? has it overfitted?\n",
    "\n",
    "## Retrain your network until you think the model has trained properly (i.e. not underfitting, not overfitting, just right ðŸ˜Š. Write down the 3 epoch values! One for underfitting, one for overfitting, and one for just right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11393c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa40b1",
   "metadata": {},
   "source": [
    "# ðŸ›‘âœ‹ STOP âœ‹ðŸ›‘ only once you've trained your model propely should you continue onto the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aa5cfb",
   "metadata": {},
   "source": [
    "## Now visualize your trained original data and trained reconstructions on the same plot with 2D PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d39015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2adef9",
   "metadata": {},
   "source": [
    "## Now visualize your test original data and test reconstructions on the same plot with 2D PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter coer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a1f5e",
   "metadata": {},
   "source": [
    "# Do you notice anything interesting about these plots?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
