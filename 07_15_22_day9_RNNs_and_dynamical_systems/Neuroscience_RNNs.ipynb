{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neuroscience-RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNNs\n",
        "Training RNNs on the Mante-Susillo context-dependent integration (CDI) task."
      ],
      "metadata": {
        "id": "j1WGO9mhDfYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bZFa7OUOzpof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of"
      ],
      "metadata": {
        "id": "A77O07aPzQVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necesary modules\n",
        "import numpy as np\n",
        "from random import randrange\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "aSOnzMk-Eoi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of how to build NN in PyTorch"
      ],
      "metadata": {
        "id": "XglKmPEozVGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a simple RNN class\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(RNN,self).__init__()\n",
        "    # initialize RNN\n",
        "  def forward(x):\n",
        "    # forward propogate inputs"
      ],
      "metadata": {
        "id": "bsxoSzh1zYoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Add a section on how to build different RNN architectures in python\n",
        "\n",
        "Add some intuition of BPTT\n",
        "\n",
        "Add some inutition on the pros/cons of these methods for neuroscience\n"
      ],
      "metadata": {
        "id": "prpXi2MX2xrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of how Torch handles backpropogation and writing a simple training loop"
      ],
      "metadata": {
        "id": "_sy6lCImzPre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple traiing loop"
      ],
      "metadata": {
        "id": "iPQrtBRNz6Ee"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build an RNN"
      ],
      "metadata": {
        "id": "RIMkRO4PDmnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    # recently changed var by normalizing it by N, before was 0.045 w/o normilazation\n",
        "    def __init__(self, hyperParams, task, device='cuda:0'):\n",
        "        #self._device = torch.device(\"cpu\")\n",
        "        #if torch.cuda.is_available():\n",
        "        self._device = torch.device(device)\n",
        "        super(RNN, self).__init__()                                            # initialize parent class\n",
        "        self._inputSize = int(hyperParams[\"inputSize\"])\n",
        "        self._hiddenSize = int(hyperParams[\"hiddenSize\"])\n",
        "        self._outputSize = int(hyperParams[\"outputSize\"])\n",
        "        self._g = hyperParams[\"g\"]\n",
        "        self._hiddenInitScale = hyperParams[\"initScale\"]                       # sets tolerance for determining validation accuracy# initializes hidden layer                                                   # sets noise for hidden initialization\n",
        "        self._dt= hyperParams[\"dt\"]\n",
        "        self._batchSize = int(hyperParams[\"batchSize\"])\n",
        "        self._hParams = hyperParams                                             # used for saving training conditions\n",
        "        self._init_hidden()   \n",
        "        self._totalTrainTime = 0                                               # accumulates training time\n",
        "        self._timerStarted = False\n",
        "        self._useForce = False            # if set to true this slightly changes the forward pass \n",
        "        self._useHeb = False\n",
        "        self._fixedPoints = []\n",
        "        self._tol = 1\n",
        "        \n",
        "        self._J = {\n",
        "        'in' : torch.randn(self._hiddenSize, self._inputSize).to(self._device)*(1/2),\n",
        "        'rec' : ((self._g**2)/50)*torch.randn(self._hiddenSize, self._hiddenSize).to(self._device),\n",
        "        'out' : (0.1*torch.randn(self._outputSize, self._hiddenSize).to(self._device)),\n",
        "        'bias' : torch.zeros(self._hiddenSize, 1).to(self._device)*(1/2)\n",
        "        }\n",
        "\n",
        "\n",
        "        try:\n",
        "            self._use_ReLU = int(hyperParams[\"ReLU\"])             # determines the activation function to use\n",
        "        except:\n",
        "            self._use_ReLU = 0\n",
        "            \n",
        "\n",
        "        self._task = task\n",
        "\n",
        "              \n",
        "\n",
        "        #create an activity tensor that will hold a history of all hidden states\n",
        "        self._activityTensor = np.zeros((50))\n",
        "        self._neuronIX = None    #will hold neuron sorting from TCA\n",
        "        self._targets=[]\n",
        "        self._losses = 0     #trainer should update this with a list of training losses\n",
        "        self._MODEL_NAME = 'models/UNSPECIFIED MODEL'   #trainer should update this\n",
        "        self._pca = []\n",
        "        self._recMagnitude = []     #will hold the magnitude of reccurent connections at each step of training\n",
        "        # will hold the previous weight values\n",
        "        self._w_hist = []    # will hold the previous weight values\n",
        "\n",
        "        self._valHist = []        # empty list to hold history of validation accuracies\n",
        "\n",
        "        #self.fractions = []\n",
        "        #weight matrices for the RNN are initialized with random normal\n",
        "\n",
        "    def _startTimer(self):\n",
        "        '''\n",
        "        starts timer for training purposes\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None.\n",
        "\n",
        "        '''\n",
        "        self._tStart = time.time()\n",
        "        self._timerStarted = True\n",
        "    \n",
        "    def _endTimer(self):\n",
        "        '''\n",
        "        stops training timer\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None.\n",
        "\n",
        "        '''\n",
        "        if self._timerStarted == False:\n",
        "            return\n",
        "        else:\n",
        "            self._totalTrainTime += time.time() - self._tStart\n",
        "            self._timerStarted = False\n",
        "    \n",
        "    def setName(self, name):\n",
        "        '''\n",
        "        sets name of model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        name : string\n",
        "            model will be saved as a .pt file with this name in the /models/ directory.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None.\n",
        "\n",
        "        '''\n",
        "        self._MODEL_NAME = \"models/\" + name\n",
        "        \n",
        "    def createValidationSet(self, test_iters=2000):\n",
        "        '''\n",
        "        DESCRIPTION:\n",
        "        Creates the validation dataset which will be used to \n",
        "        decide when to terminate RNN training. The validation \n",
        "        dataset consists of means sampled uniformly from -0.1875 \n",
        "        to +0.1875. The variance of all instances in the validation \n",
        "        dataset is equal to the variance of the trainign dataset \n",
        "        as specified by the task object.\n",
        "        \n",
        "        validation dataset has shape (test_iters, numInputs, tSteps)\n",
        "        \n",
        "        PARAMETERS:\n",
        "        **valSize: specifies the size of the validation dataset to use\n",
        "        **task: task object that is used to create the validation data \n",
        "        set\n",
        "        '''\n",
        "        # initialize tensors to hold validation data\n",
        "        breakpoint()\n",
        "        self.validationData = torch.zeros(test_iters, self._inputSize,  self._task.N).to(self._device)\n",
        "        self.validationTargets = torch.zeros(test_iters,1).to(self._device)\n",
        "        # means for validation data\n",
        "        meanValues = np.linspace(0, 0.1875, 20)\n",
        "        for trial in range(test_iters):\n",
        "            # to get genetic and bptt different I divided by 30\n",
        "            mean_overide = meanValues[trial %20]\n",
        "            inpt_tmp, condition_tmp = self._task.GetInput(mean_overide=mean_overide)\n",
        "            self.validationData[trial,:, :] = inpt_tmp.t()\n",
        "            self.validationTargets[trial] = condition_tmp\n",
        "        print('Validation dataset created!\\n')\n",
        "\n",
        "    def GetValidationAccuracy(self, test_iters=2000):\n",
        "        '''\n",
        "        Will get validation accuracy of the model on the specified task to \n",
        "        determine if training should be terminated\n",
        "        '''\n",
        "        accuracy = 0.0\n",
        "        tol = self._tol\n",
        "\n",
        "        inpt_data = self.validationData#.t()\n",
        "        condition = self.validationTargets\n",
        "        condition = torch.squeeze(condition)\n",
        "        output = self.feed(inpt_data)\n",
        "        output_final = torch.sign(output[-1,:])\n",
        "        # scale ouput in the case of multisensory network\n",
        "        # compute the difference magnitudes between model output and target output\n",
        "        #if isinstance(self._task, DMC):\n",
        "        #    differences = self.validationTargets-output_final.view(-1,1)\n",
        "        #    num_errors = torch.sum(torch.abs(differences) > 0.15).item()\n",
        "        #else:\n",
        "        error = torch.abs(condition-output_final)\n",
        "        # threshold this so that errors greater than tol(=0.1) are counted\n",
        "        num_errors = torch.where(error>tol)[0].shape[0]\n",
        "        accuracy = (test_iters - num_errors) / test_iters\n",
        "\n",
        "        \n",
        "        self._valHist.append(accuracy)                 # appends current validation accuracy to history\n",
        "    \n",
        "        \n",
        "        return accuracy\n",
        "\n",
        "    def _UpdateHidden(self, inpt):\n",
        "        '''Updates the hidden state of the RNN. NOTE: The way the hidden state is updated \n",
        "        will vary depending on the task and learning rule the RNN was trained with. \n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inpt : PyTorch cuda tensor\n",
        "            inputs to the network at current time step. Has shape (inputSize, batchSize)\n",
        "        use_relu : BOOL, optional\n",
        "            when true, the network will use ReLU activations. The default is False.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        hidden_next : PyTorch cuda Tensor\n",
        "            Tensor of the updated neuron activations. Has shape (hiddenSize, batchSize)\n",
        "\n",
        "        '''\n",
        "        dt = self._dt\n",
        "        Jin = self._J[\"in\"]\n",
        "\n",
        "        # set activation function\n",
        "        if self._use_ReLU:\n",
        "            act_func = nn.functional.relu\n",
        "        else:\n",
        "            act_func = lambda x : 1+torch.tanh(x)\n",
        "\n",
        "        # forward pass through the network\n",
        "        noiseTerm=0\n",
        "        hidden_next = dt*torch.matmul(Jin, inpt) + \\\n",
        "        dt*torch.matmul(self._J['rec'], (1+torch.tanh(self._hidden))) + \\\n",
        "        (1-dt)*self._hidden + dt*self._J['bias'] + 0*noiseTerm\n",
        "\n",
        "        self._hidden = hidden_next        # updates hidden layer\n",
        "        return hidden_next\n",
        "\n",
        "    def _forward(self, inpt):\n",
        "        '''\n",
        "        Computes the RNNs forward pass activations for a single timestep\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inpt : PyTorch cuda Tensor \n",
        "            inputs to the network for the current timestep. Shape (inputSize, batchSize)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        output : PyTorch cuda Tensor\n",
        "            output of the network after this timestep. Has shape (outputSize, batchSize)\n",
        "        PyTorch cuda Tensor\n",
        "            copy of the hidden state activations after the forward pass. Has shape\n",
        "            (hiddenSize, batchSize)\n",
        "\n",
        "        '''\n",
        "\n",
        "        #ensure the input is a torch tensor\n",
        "        if torch.is_tensor(inpt) == False:\n",
        "            inpt = torch.from_numpy(inpt).float()                              # inpt must have shape (1,1)\n",
        "        inpt = inpt.reshape(self._inputSize, -1)\n",
        "        \n",
        "        # compute the forward pass\n",
        "        self._UpdateHidden(inpt)\n",
        "        if self._useHeb:\n",
        "            output = torch.tanh(self._hidden[0])\n",
        "        elif self._useForce:\n",
        "            output = torch.tanh(torch.matmul(self._J['out'], torch.tanh(self._hidden)))\n",
        "        else:\n",
        "            output = torch.matmul(self._J['out'], self._hidden)\n",
        "\n",
        "        return output, self._hidden.clone()\n",
        "\n",
        "    def feed(self, inpt_data, return_hidden=False, return_states=False):\n",
        "        '''\n",
        "        Feeds an input data sequence into an RNN\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inpt_data : PyTorch CUDA Tensor\n",
        "            Inputs sequence to be fed into RNN. Has shape (batchSize, inputSize, Time)\n",
        "        return_hidden : BOOL, optional\n",
        "            When True, the hidden states of the RNN are returned as list of length\n",
        "            Time where each element is a NumPy array of shape (hiddenSize, batchSize)\n",
        "            containing the hidden state activations through the course of the input\n",
        "            sequence. The default is False.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        output_trace : PyTorch CUDA Tensor\n",
        "            output_trace: output of the network over all timesteps. Will have shape \n",
        "            (batchSize, inputSize) i.e. 40x1 for single sample inputs\n",
        "        hidden_states : PyTorch CUDA Tensor\n",
        "            hidden_states: hidden states of the network through a trial, has shape\n",
        "            (batch_size, time_steps, hidden_size)\n",
        "\n",
        "        '''\n",
        "\n",
        "        #num_inputs = len(inpt_data[0])\n",
        "        batch_size = inpt_data.shape[0]\n",
        "        inpt_seq_len = inpt_data.shape[-1]\n",
        "        assert inpt_data.shape[1] == self._inputSize, \"Size of inputs:{} does not match network's input size:{}\".format(inpt_data.shape[1], self._inputSize)\n",
        "        num_t_steps = inpt_data.shape[2]\n",
        "        \n",
        "        output_trace = torch.zeros(num_t_steps, batch_size).to(self._device)\n",
        "        if return_hidden:\n",
        "            hidden_trace = []\n",
        "        if return_states:\n",
        "            hidden_states = torch.zeros((batch_size, inpt_seq_len, self._hiddenSize), requires_grad=True)\n",
        "\n",
        "        self._init_hidden(numInputs=batch_size)  # initializes hidden state\n",
        "        inpt_data = inpt_data.permute(2,1,0)     # now has shape TxMxB\n",
        "        for t_step in range(len(inpt_data)):\n",
        "            output, hidden = self._forward(inpt_data[t_step])\n",
        "            if return_hidden:\n",
        "                hidden_trace.append(hidden.cpu().detach().numpy())      # unsure if there are any dependencies on hidden_trace\n",
        "\n",
        "            if return_states:\n",
        "                hidden_states[:,t_step,:] = hidden.T                    # (batch_size, hidden_size)\n",
        "                \n",
        "            if self._useHeb:\n",
        "                output_trace[t_step,:] = hidden.detach()[0]\n",
        "            else:\n",
        "                output_trace[t_step,:] = output\n",
        "        if return_hidden:\n",
        "            hh = np.array(hidden_trace)\n",
        "            return output_trace, hh\n",
        "            \n",
        "        if return_states:\n",
        "            return output_trace, hidden_states\n",
        "        #print('shape of output trace', len(output_trace[0]))\n",
        "        return output_trace\n",
        "        \n",
        "\n",
        "    def save(self, N=\"\", tElapsed=0, *kwargs):\n",
        "        '''\n",
        "        saves RNN parameters and attributes. User may define additional attributes\n",
        "        to be saved through kwargs\n",
        "        '''\n",
        "        print('valdiation history', self._valHist)\n",
        "        if N==\"\":     # no timestamp\n",
        "            model_name = self._MODEL_NAME+'.pt'\n",
        "            print(\"model name: \", model_name)\n",
        "        else:         # timestamp\n",
        "            model_name = self._MODEL_NAME + '_' + str(N) + '.pt'\n",
        "        if tElapsed==0:\n",
        "            torch.save({'weights': self._J, \\\n",
        "                        'weight_hist':self._w_hist, \\\n",
        "                        'activities': self._activityTensor, \\\n",
        "                        'targets': self._targets, \\\n",
        "                        'pca': self._pca, \\\n",
        "                        'losses': self._losses, \\\n",
        "                        'rec_magnitude' : self._recMagnitude, \\\n",
        "                        'neuron_idx': self._neuronIX,\\\n",
        "                        'validation_history' : self._valHist,\n",
        "                        'fixed_points': self._fixedPoints}, model_name)\n",
        "                \n",
        "            # save model hyper-parameters to text file\n",
        "            f = open(self._MODEL_NAME+\".txt\",\"w\")\n",
        "            for key in self._hParams:\n",
        "                f.write( str(key)+\" : \"+str(self._hParams[key]) + '\\n')\n",
        "            f.write( \"total training time: \" + str(self._totalTrainTime) + '\\n')\n",
        "            f.close()\n",
        "            \n",
        "        else:\n",
        "            torch.save({'weights': self.J, \\\n",
        "                        'weight_hist':self.w_hist, \\\n",
        "                        'activities': self.activity_tensor, \\\n",
        "                        'targets': self.targets, \\\n",
        "                        'pca': self.pca, \\\n",
        "                        'losses': self.losses, \\\n",
        "                        'rec_magnitude' : self.rec_magnitude, \\\n",
        "                        'neuron_idx': self.neuron_idx, \\\n",
        "                        'fractions' : self.fractions, \\\n",
        "                        'validation_history' : self.valHist, \\\n",
        "                        'tElapsed' : tElapsed,\n",
        "                        'fixed_points' : self._fixedPoints}, model_name)\n",
        "        \n",
        "        #torch.save({'weights': self.J, 'targets': self.targets,  'losses': self.losses,'validation_history' : self.valHist}, model_name)\n",
        "\n",
        "    def load(self, model_name, *kwargs):\n",
        "        '''\n",
        "        Loads in parameters and attributers from a previously instantiated model.\n",
        "        User may define additional model attributes to load through kwargs\n",
        "        '''\n",
        "        # add file suffix to model_name\n",
        "        fname = model_name+'.pt'\n",
        "        model_dict = torch.load(fname)\n",
        "        # load attributes in model dictionary\n",
        "        if 'weights' in model_dict:\n",
        "            self._J = model_dict['weights']\n",
        "            for layer in self._J:  # move weights to correct device\n",
        "                self._J[layer] = self._J[layer].to(self._device)\n",
        "        else:\n",
        "            print('WARNING!! NO WEIGHTS FOUND\\n\\n')\n",
        "        if 'activities' in model_dict:\n",
        "            self._activityTensor = model_dict['activities']\n",
        "        else:\n",
        "            print('WARNING!! NO ACTIVITIES FOUND\\n\\n')\n",
        "        if 'targets' in model_dict:\n",
        "            self._targets = model_dict['targets']\n",
        "        else:\n",
        "            print('WARNING!! NO TARGETS FOUND\\n\\n')\n",
        "        if 'pca' in model_dict:\n",
        "            self._pca = model_dict['pca']\n",
        "        else:\n",
        "            print('WARNING!! NO PCA DATA FOUND\\n\\n')\n",
        "        if 'losses' in model_dict:\n",
        "            self._losses = model_dict['losses']\n",
        "        else:\n",
        "            print('WARNING!! NO LOSS HISTORY FOUND\\n\\n')\n",
        "        if 'validation_history' in model_dict:\n",
        "            self._valHist = model_dict['validation_history']\n",
        "        else:\n",
        "            print('WARNING!! NO VALIDATION HISTORY FOUND\\n\\n')\n",
        "            \n",
        "        if 'fixed_points' in model_dict:\n",
        "            self._fixedPoints = model_dict['fixed_points']\n",
        "            \n",
        "        # try to load additional attributes specified for kwargs\n",
        "        for key in kwargs:\n",
        "            print('loading of', key, 'has not yet been implemented!')\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        if 'rec_magnitude' in model_dict:\n",
        "            self.rec_magnitude = model_dict['rec_magnitude']\n",
        "        else:\n",
        "            print('WARNING!! NO WEIGHT HISTORY FOUND\\n\\n')\n",
        "        if 'neuron_idx' in model_dict:\n",
        "            self.neuron_idx = model_dict['neuron_idx']\n",
        "        else:\n",
        "            print('WARNING!! NO NEURON INDEX FOUND\\n\\n')\n",
        "\n",
        "        print('\\n\\n')\n",
        "        print('-'*50)\n",
        "        print('-'*50)\n",
        "        print('RNN model succesfully loaded ...\\n\\n')\n",
        "\n",
        "\n",
        "    # maybe I should consider learning the initial state?\n",
        "    def _init_hidden(self, numInputs=1):\n",
        "        self._hidden = self._hiddenInitScale*(torch.randn(self._hiddenSize, numInputs).to(self._device))\n",
        "    \n",
        "    def GetF(self):\n",
        "        W_rec = self._J['rec'].data.cpu().detach().numpy()\n",
        "        W_in = self._J['in'].data.cpu().detach().numpy()\n",
        "        b = self._J['bias'].data.cpu().detach().numpy()\n",
        "        ReLU_flag = self._use_ReLU\n",
        "\n",
        "        def master_function(inpt, relu=ReLU_flag):\n",
        "            dt = 0.1\n",
        "            sizeOfInput = len(inpt)\n",
        "            inpt = inpt.reshape(sizeOfInput,1)\n",
        "            if relu:\n",
        "                return lambda x: np.squeeze( dt*np.matmul(W_in, inpt) + dt*np.matmul(W_rec, (np.maximum( np.zeros((self._hiddenSize,1)), x.reshape(self._hiddenSize,1)) )) - dt*x.reshape(self._hiddenSize,1) + b*dt)\n",
        "            else:\n",
        "                if self._useHeb: #TODO: update this to incorporate bias\n",
        "                    def update_fcn(x):\n",
        "                        x[1] = 1       # Bias from Miconi 2017\n",
        "                        x[10] = 1\n",
        "                        x[11] = -1\n",
        "                        x = np.squeeze( dt*np.matmul(W_in, inpt) + dt*np.matmul(W_rec, (np.tanh(x.reshape(self._hiddenSize,1)))) - dt*x.reshape(self._hiddenSize,1) + b*dt)\n",
        "                        x[1] = 1       # Bias from Miconi 2017\n",
        "                        x[10] = 1\n",
        "                        x[11] = -1\n",
        "                        return x\n",
        "                    #return lambda x: np.squeeze( dt*np.matmul(W_in, inpt) + dt*np.matmul(W_rec, (np.tanh(x.reshape(self._hiddenSize,1)))) - dt*x.reshape(self._hiddenSize,1) + b*dt)\n",
        "                    return update_fcn\n",
        "                elif self._useForce:\n",
        "                    return lambda x: np.squeeze( dt*np.matmul(W_in, inpt) + dt*np.matmul(W_rec, (np.tanh(x.reshape(self._hiddenSize,1)))) - dt*x.reshape(self._hiddenSize,1) + b*dt)\n",
        "                else:  # BPTT and GA RNNs\n",
        "                    return lambda x: np.squeeze( dt*np.matmul(W_in, inpt) + dt*np.matmul(W_rec, (1+np.tanh(x.reshape(self._hiddenSize,1)))) - dt*x.reshape(self._hiddenSize,1) + b*dt)\n",
        "\n",
        "        return master_function\n",
        "        \n",
        "    def plotLosses(self):\n",
        "        plt.plot(self._losses)\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Trial')\n",
        "        plt.title(self._MODEL_NAME)"
      ],
      "metadata": {
        "id": "bH2wpE77DhoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Data Loader"
      ],
      "metadata": {
        "id": "0MRcJrjfDqNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class for generating data \n",
        "class ContextDependentIntegration():\n",
        "    '''\n",
        "    This class simulates the Mante and Sussillo 2013 context-dependent \n",
        "    integration task described here:\n",
        "\n",
        "    https://www.nature.com/articles/nature12742\n",
        "    '''\n",
        "\n",
        "    N_INPUT_CHANNELS = 2\n",
        "\n",
        "    def __init__(self, N=750, mean=0.1857, var=1, device=\"cuda:0\"):\n",
        "        # determines if code should run on CPU or GPU\n",
        "        if torch.cuda.is_available() and device==\"cuda:0\":\n",
        "            self._device = torch.device(device)\n",
        "        else:\n",
        "            self._device = torch.device(\"cpu\")\n",
        "\n",
        "        self.N = N                 # number of time steps in a trial\n",
        "        self._max_mean = mean      # maximum mean value of signal\n",
        "        self._var = var            # variance of signal\n",
        "        #self._version = \"\"        # is this deprecated ??\n",
        "        #self._name = \"Ncontext\"   # is this deprecated ??\n",
        "\n",
        "    def _random_generate_input(self, mean, trial_prob=0.5):\n",
        "        '''\n",
        "        Generates a 1D noisy signal for a context with the provided mean. The \n",
        "        chance of a positive trial is given by trial_prob.\n",
        "        '''\n",
        "        if torch.rand(1).item() < trial_prob:\n",
        "            return mean*torch.ones(self.N)\n",
        "        return -mean*torch.ones(self.N)\n",
        "        \n",
        "    def GetInput(self, mean_overide=None, var_overide=None):\n",
        "        '''\n",
        "        Generates a single trial of data for the CDI-task. Returns the\n",
        "        perceptual data (inpts) and the labels (target)\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean_overide : TYPE, optional\n",
        "            DESCRIPTION. The default is 1.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        inpts : PyTorch CUDA Tensor\n",
        "            DESCRIPTION.\n",
        "        target : TYPE\n",
        "            DESCRIPTION.\n",
        "        '''\n",
        "        inpts = torch.zeros((self.N, 2*self.N_INPUT_CHANNELS)).to(self._device)\n",
        "        \n",
        "        # sample trial mean from ~U[-max_mean, max_mean]\n",
        "        mean = torch.rand(1).item() * self._max_mean\n",
        "\n",
        "        # allows caller to use a deterministic mean for this trial\n",
        "        if mean_overide is not None:\n",
        "            mean = mean_overide\n",
        "\n",
        "        # allows caller to override the default noise\n",
        "        if var_overide is not None:\n",
        "            var = var_overide\n",
        "        else:\n",
        "            var = self._var  \n",
        "\n",
        "        # randomly sets one of the channels to be on\n",
        "        go_channel = randrange(self.N_INPUT_CHANNELS)\n",
        "        for channel_num in range(self.N_INPUT_CHANNELS):\n",
        "            inpts[:,channel_num] = self._random_generate_input(mean)\n",
        "            # generate the GO signals\n",
        "            if go_channel == channel_num:\n",
        "                inpts[:, self.N_INPUT_CHANNELS + channel_num] = 1\n",
        "                target = torch.sign(torch.mean(inpts[:, channel_num]))\n",
        "            else:\n",
        "                inpts[:, self.N_INPUT_CHANNELS + channel_num] = 0\n",
        "\n",
        "        \n",
        "        # adds noise to inputs\n",
        "        inpts[:,:self.N_INPUT_CHANNELS] += var*torch.randn(self.N, \n",
        "            self.N_INPUT_CHANNELS).to(self._device)\n",
        "        return inpts, target\n",
        "      "
      ],
      "metadata": {
        "id": "eggNkeS8Dpka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize a few trials\n",
        "training_task = ContextDependentIntegration()\n",
        "\n",
        "'''for sample_ix in range(10):\n",
        "    sample_trial, sample_target = training_task.GetInput(mean_overide=1, \n",
        "                                                         var_overide=0.25)\n",
        "\n",
        "\n",
        "    plt.figure(sample_ix)\n",
        "    plt.plot(sample_trial.cpu()[:,0])\n",
        "    plt.plot(sample_trial.cpu()[:,1])\n",
        "    plt.plot(sample_trial.cpu()[:,2])\n",
        "    plt.plot(sample_trial.cpu()[:,3])\n",
        "    plt.legend([\"context 1\", \"context 2\", \"GO 1\", \"GO 2\"])\n",
        "    plt.title(\"Goal: \" + str(sample_target.item()))'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "P15t4nB5ETln",
        "outputId": "f0699ff3-41bd-4765-8fe1-fc5681f5d7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for sample_ix in range(10):\\n    sample_trial, sample_target = training_task.GetInput(mean_overide=1, \\n                                                         var_overide=0.25)\\n\\n\\n    plt.figure(sample_ix)\\n    plt.plot(sample_trial.cpu()[:,0])\\n    plt.plot(sample_trial.cpu()[:,1])\\n    plt.plot(sample_trial.cpu()[:,2])\\n    plt.plot(sample_trial.cpu()[:,3])\\n    plt.legend([\"context 1\", \"context 2\", \"GO 1\", \"GO 2\"])\\n    plt.title(\"Goal: \" + str(sample_target.item()))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nOthPz7rETn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M0s7bf9LETqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OSvi_EYzETx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lm79WSiMET0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a training loop"
      ],
      "metadata": {
        "id": "DMrAJ83CDsjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PRINT_EVERY = 10\n",
        "\n",
        "class BPTT(RNN):\n",
        "    '''create a trainer object that will be used to trian an RNN object'''\n",
        "    def __init__(self, hyperParams, task, lr=5e-4):\n",
        "        super(BPTT, self).__init__(hyperParams, task)\n",
        "        '''\n",
        "        description of parameters:\n",
        "\n",
        "        '''\n",
        "\n",
        "        self._num_epochs = 2_000\n",
        "        self._learning_rate = lr   \n",
        "        self._hParams[\"learning_rate\"] = self._learning_rate\n",
        "        \n",
        "        # cast as PyTorch variables\n",
        "        self._J['in'] = Variable(self._J['in'], requires_grad=True)\n",
        "        self._J['rec'] = Variable(self._J['rec'], requires_grad=True)\n",
        "        self._J['out'] = Variable(self._J['out'], requires_grad=True)\n",
        "        \n",
        "        #self._params = [self._J['in'], self._J['rec'], self._J['out']]\n",
        "        self._params = [self._J['rec']]\n",
        "        self._optimizer = torch.optim.Adam(self._params, lr=self._learning_rate)\n",
        "\n",
        "\n",
        "        \n",
        "        self.all_losses = []\n",
        "        \n",
        "        self._hidden = Variable(self._hidden, requires_grad=True)\n",
        "\n",
        "\n",
        "    def loss_fn(self, target, output, hidden_states=0):\n",
        "\n",
        "        # extract start and end of output\n",
        "        y_end = output[-1]\n",
        "        y_strt = output[0]\n",
        "\n",
        "        # use loss from Mante 2013\n",
        "        squareLoss = (y_end-torch.sign(target.T))**2 + (y_strt - 0)**2\n",
        "        SquareLoss = torch.sum( squareLoss, axis=0 )\n",
        "        return SquareLoss  \n",
        "\n",
        "\n",
        "    def train_one_batch(self, input, trial, condition):\n",
        "        '''\n",
        "        trains a model on a single batch of trials\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input : PyTorch A Tensor\n",
        "            Inputs sequence to be fed into RNN. Has shape (batchSize, sequence_len, inputSize)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        output : PyTorch CUDA Tensor\n",
        "        '''\n",
        "\n",
        "        batch_size, inpt_seq_len, input_size = input.shape      # input dimensions\n",
        "\n",
        "        #create an activities tensor for the rnn_model\n",
        "        self._optimizer.zero_grad()\n",
        "        #self.StoreRecMag()\n",
        "\n",
        "        output = torch.zeros((self._task.N, batch_size*self._outputSize)) \n",
        "        output_temp = torch.Tensor([0])\n",
        "\n",
        "        trial_length = self._task.N\n",
        "        #hidden_states = torch.zeros((batch_size, inpt_seq_len, self._hiddenSize), requires_grad=True)\n",
        "        for i in range(trial_length): \n",
        "            inputNow = input[:,i,:].t()\n",
        "            output_temp, hidden = self._forward(inputNow)           #I need to generalize this line to work for context task\n",
        "            #hidden_states[:,i,:] = hidden.T                         # (batch_size, hidden_size)\n",
        "            #output_temp, hidden = self.rnn_model.forward(input[:,i], hidden, dt)             #this incridebly hacky must improve data formatting accross all modules to correctly implement a context task that doesn't clash with DM task\n",
        "            output[i] = np.squeeze(output_temp)\n",
        "            if (i %10 == 0):\n",
        "                activityIX = int(i/10)\n",
        "                self._activityTensor[self.trial_count, activityIX, :] = np.squeeze(torch.tanh(self._hidden).cpu().detach().numpy())[:,0]\n",
        "        self.trial_count += 1\n",
        "        # self.activity_tensor[trial, i, :] = hidden.detach().numpy()  # make sure calling detach does not mess with the backprop gradients (I think .data does)\n",
        "        # https://pytorch.org/docs/stable/autograd.html\n",
        "        #pdb.set_trace()\n",
        "        loss = self.loss_fn(condition, output)\n",
        "        loss.backward()\n",
        "        self._optimizer.step()\n",
        "        \n",
        "        return output, loss.item()\n",
        "\n",
        "    \n",
        "    def getBatch(self):\n",
        "        x_batch = torch.zeros((self._batchSize, self._task.N, self._inputSize))\n",
        "        #x_batch = torch.zeros((750, self._inputSize, self._batchSize))\n",
        "        y_batch = torch.zeros(self._batchSize)\n",
        "        for dataPtIX in range(self._batchSize):\n",
        "            inpt, condition = self._task.GetInput()\n",
        "            x_batch[dataPtIX,:,:] = inpt\n",
        "            y_batch[dataPtIX] = condition\n",
        "        return x_batch, y_batch\n",
        "\n",
        "\n",
        "    def train(self, termination_accuracy=0.9):\n",
        "        self._startTimer()\n",
        "        # pre-generate a set of validation trials that will be constant throughout training\n",
        "        self.createValidationSet()\n",
        "        # create activity tensor\n",
        "        self._activityTensor = np.zeros((self._num_epochs, int(self._task.N/10), self._hiddenSize))\n",
        "\n",
        "        \n",
        "        # inps_save = np.zeros((num_epochs, trial_length))\n",
        "        self.trial_count=0\n",
        "        self.targets = []   #will hold target output for each trial\n",
        "        \n",
        "        validation_accuracy = 0.0\n",
        "        validation_acc_hist = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "        meanLossHist = 100*np.ones(20)\n",
        "        # empty list that will hold history of validation accuracy\n",
        "        loss_hist = []\n",
        "        \n",
        "        # for CUDA implementation\n",
        "        inpt = Variable(torch.zeros(int(self._batchSize), self._task.N, self._inputSize).to(self._device))\n",
        "        \n",
        "        #trial = 0\n",
        "        loss = np.inf\n",
        "        \n",
        "        # start main training loop\n",
        "        while(validation_accuracy < termination_accuracy):\n",
        "            # terminate training after maximum allowed epochs\n",
        "            if self.trial_count >= self._num_epochs:\n",
        "                break\n",
        "            \n",
        "            # periodically prints training status\n",
        "            if self.trial_count %PRINT_EVERY == 0:\n",
        "                print('trial #:', self.trial_count)\n",
        "                print('validation accuracy', validation_accuracy)\n",
        "                print(\"validation history\", validation_acc_hist)\n",
        "            #print('loss', loss)\\\n",
        "            \n",
        "            \n",
        "            # generate a batch of training trials\n",
        "            # I should move the getBatch method to the data class\n",
        "            inpt[:], condition = self.getBatch()    # inpt has shape 750x1\n",
        "            self.targets.append(condition[-1].item())\n",
        "            condition = Variable(condition)\n",
        "            \n",
        "            # train model on this data    \n",
        "            self._init_hidden()   # resets the hidden state for new trials\n",
        "            # train the network on this batch of trials\n",
        "            output, loss = self.train_one_batch(inpt, self.trial_count, condition)\n",
        "            \n",
        "            # append current loss to history\n",
        "            self.all_losses.append(loss)\n",
        "\n",
        "            validation_accuracy_curr = self.GetValidationAccuracy()\n",
        "            loss_hist.append(1-validation_accuracy_curr)\n",
        "            #print('loss hist', np.mean(np.diff(meanLossHist)))\n",
        "            validation_acc_hist[:9] = validation_acc_hist[1:]\n",
        "            validation_acc_hist[-1] = validation_accuracy_curr\n",
        "            meanLossHist[:19] = meanLossHist[1:]\n",
        "            meanLossHist[-1] = np.mean(self.all_losses[-20:])\n",
        "            validation_accuracy = np.min(validation_acc_hist)\n",
        "                 \n",
        "            \n",
        "            # save the model every 100 trials\n",
        "            # why am I saving the model twice?\n",
        "            if self.trial_count %100 == 0:\n",
        "                self.saveProgress()\n",
        "            \n",
        "        self._targets = np.array(self.targets)        #hacky\n",
        "        self._losses = np.array(self.all_losses)#self.all_losses      #also hacky\n",
        "        self._activityTensor = self._activityTensor[:self.trial_count,:,:]\n",
        "        print('shape of activity tensor', self._activityTensor.shape)  \n",
        "        print('trial count', self.trial_count)\n",
        "        self._endTimer()\n",
        "\n",
        "\n",
        "    def saveProgress(self):\n",
        "        self._targets = self.targets        #hacky\n",
        "        self._losses = np.array(self.all_losses)#self.all_losses      #also hacky\n",
        "        #self.rnn_model.activity_tensor = self.rnn_model.activity_tensor[:self.trial_count,:,:]\n",
        "        self.save()\n",
        "        print('model back-ed up')"
      ],
      "metadata": {
        "id": "1ZMfScbKDt9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyper_params = {                  # dictionary of all RNN hyper-parameters\n",
        "   \"inputSize\" : 4,\n",
        "   \"hiddenSize\" : 50,\n",
        "   \"outputSize\" : 1,\n",
        "   \"g\" : 1 ,\n",
        "   \"inputVariance\" : 0.5,\n",
        "   \"outputVariance\" : 0.5,\n",
        "   \"biasScale\" : 0,\n",
        "   \"initScale\" : 0.3,\n",
        "   \"dt\" : 0.1,\n",
        "   \"batchSize\" : 500,\n",
        "   \"taskMean\" : 0.1857,\n",
        "   \"taskVar\" : 1,\n",
        "   \"ReLU\" : 0\n",
        "   }\n",
        "\n",
        "my_rnn = BPTT(hyper_params, training_task)"
      ],
      "metadata": {
        "id": "UJVgnxoVTd8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_rnn.setName(\"tmp\")\n",
        "import time"
      ],
      "metadata": {
        "id": "dHMGE6Z0VSaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_rnn.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j786aFh7Vcun",
        "outputId": "dee645b3-aa8b-4a87-fd99-ea26344ac905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 332, in set_trace\n",
            "    sys.settrace(self.trace_dispatch)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> <ipython-input-2-1925a9166785>(120)createValidationSet()\n",
            "-> self.validationData = torch.zeros(test_iters, self._inputSize,  self._task.N).to(self._device)\n",
            "(Pdb) c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.7/bdb.py\", line 343, in set_continue\n",
            "    sys.settrace(None)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation dataset created!\n",
            "\n",
            "trial #: 0\n",
            "validation accuracy 0.0\n",
            "validation history [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2318.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trial #: 10\n",
            "validation accuracy 0.504\n",
            "validation history [0.511, 0.5125, 0.5095, 0.508, 0.504, 0.505, 0.507, 0.507, 0.5045, 0.5045]\n",
            "trial #: 20\n",
            "validation accuracy 0.4915\n",
            "validation history [0.5035, 0.503, 0.4995, 0.503, 0.5005, 0.495, 0.4915, 0.493, 0.497, 0.4955]\n",
            "trial #: 30\n",
            "validation accuracy 0.498\n",
            "validation history [0.498, 0.5045, 0.5, 0.5005, 0.504, 0.508, 0.5095, 0.521, 0.5155, 0.522]\n",
            "\n",
            "Program interrupted. (Use 'cont' to resume).\n",
            "> <ipython-input-2-1925a9166785>(193)_UpdateHidden()\n",
            "-> self._hidden = hidden_next        # updates hidden layer\n",
            "--KeyboardInterrupt--\n",
            "--KeyboardInterrupt--\n",
            "--KeyboardInterrupt--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the Results"
      ],
      "metadata": {
        "id": "VHVlK0YBDvc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# view training curves"
      ],
      "metadata": {
        "id": "Oqp1QZ2qDw8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample some behavior"
      ],
      "metadata": {
        "id": "PT7d1KLBDyza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view some trajectories in state-space"
      ],
      "metadata": {
        "id": "-pkcsUdbD0r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find attractor states"
      ],
      "metadata": {
        "id": "YHoyXbY9D2XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tSf59qXXD4M9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}